{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5281d5",
   "metadata": {},
   "source": [
    "## Set SparkSession and Feathr client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37387956",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01999d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feathr version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import feathr\n",
    "import pandas as pd\n",
    "from feathr import (\n",
    "    BOOLEAN,\n",
    "    FLOAT,\n",
    "    INPUT_CONTEXT,\n",
    "    INT32,\n",
    "    BackfillTime,\n",
    "    DerivedFeature,\n",
    "    FeathrClient,\n",
    "    Feature,\n",
    "    FeatureAnchor,\n",
    "    FeatureQuery,\n",
    "    HdfsSource,\n",
    "    MaterializationSettings,\n",
    "    ObservationSettings,\n",
    "    RedisSink,\n",
    "    TypedKey,\n",
    "    ValueType,\n",
    "    WindowAggTransformation,\n",
    ")\n",
    "from feathr.datasets.utils import maybe_download\n",
    "from feathr.utils.config import generate_config\n",
    "from feathr.utils.job_utils import get_result_df\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# PATH_TO_APP_DATA = \"hdfs://namenode:9000/data\"\n",
    "PATH_TO_APP_DATA = \"s3a://data-bucket\"\n",
    "\n",
    "print(f\"Feathr version: {feathr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2235d3",
   "metadata": {},
   "source": [
    "#### SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/26 09:34:43 WARN Utils: Your hostname, PF5L73QZ resolves to a loopback address: 127.0.1.1; using 192.168.123.113 instead (on interface wlp9s0f0)\n",
      "26/01/26 09:34:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "26/01/26 09:36:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"write-synthetic-parquet-to-hdfs\")  # type: ignore[attr-defined]\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9007\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/home/nazarov.aleksey64/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,\"\n",
    "        \"/home/nazarov.aleksey64/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.406.jar\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "    # .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.406\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c47eb4",
   "metadata": {},
   "source": [
    "#### Feathr client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e43dfb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 14:37:01.031 | INFO     | feathr.utils._env_config_reader:get:60 - Config secrets__azure_key_vault__name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-26 14:37:01.036 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__s3__s3_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-26 14:37:01.038 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__adls__adls_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-26 14:37:01.040 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__wasb__wasb_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-26 14:37:01.043 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__jdbc__jdbc_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-26 14:37:01.045 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__snowflake__snowflake_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "No offline storage enabled.\n",
      "2026-01-26 14:37:01.050 | INFO     | feathr.utils._env_config_reader:get:60 - Config spark_config__local__workspace is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 14:37:01.055 | INFO     | feathr.utils._env_config_reader:get:60 - Config feature_registry__purview__purview_name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-26 14:37:01.067 | INFO     | feathr.client:__init__:216 - Feathr client 1.0.0 initialized successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found jar file at ./feathr_2.12-1.0.0.jar\n"
     ]
    }
   ],
   "source": [
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "os.environ['REDIS_PASSWORD'] = \"\"\n",
    "\n",
    "jar_name = glob.glob(\"./*.jar\")[0]\n",
    "print(f\"Found jar file at {jar_name}\")\n",
    "\n",
    "feathr_workspace_folder = Path(\"./feathr_config.yaml\")\n",
    "\n",
    "client = FeathrClient(str(feathr_workspace_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35af48f",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601dcab",
   "metadata": {},
   "source": [
    "### Upload quick start data to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73fa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/26 09:37:06 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "26/01/26 09:37:08 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "quick_start_data_list = !ls feathr_data/\n",
    "\n",
    "for i in quick_start_data_list:\n",
    "    df_name = i.split(\".\")[0]\n",
    "    hdfs_path = f\"{PATH_TO_APP_DATA}/{df_name}\"\n",
    "\n",
    "    df = spark.createDataFrame(pd.read_csv(f\"feathr_data/{i}\"))\n",
    "    df.repartition(1).write.mode(\"overwrite\").parquet(hdfs_path)\n",
    "    \n",
    "    last_path = hdfs_path.split(\"/\")[-1]\n",
    "\n",
    "    if \"observation\" in last_path:\n",
    "        print(f\"====== {last_path} ======\")\n",
    "        spark.read.parquet(f\"{hdfs_path}\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1a218",
   "metadata": {},
   "source": [
    "### Try Feathr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08643b99",
   "metadata": {},
   "source": [
    "#### Define Feathr source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ccee1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_source = HdfsSource(\n",
    "    name=\"user_observation\",\n",
    "    path=f\"{PATH_TO_APP_DATA}/user_observation\",\n",
    "    event_timestamp_column=\"event_timestamp\",\n",
    "    timestamp_format=\"yyyy-MM-dd\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6cfd07",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90caea44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d2bcf24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03aa66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d8933d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab656807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
