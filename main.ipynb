{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5281d5",
   "metadata": {},
   "source": [
    "## Set SparkSession and Feathr client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37387956",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01999d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feathr version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import feathr\n",
    "import pandas as pd\n",
    "from feathr import (\n",
    "    BOOLEAN,\n",
    "    FLOAT,\n",
    "    INPUT_CONTEXT,\n",
    "    INT32,\n",
    "    BackfillTime,\n",
    "    DerivedFeature,\n",
    "    FeathrClient,\n",
    "    Feature,\n",
    "    FeatureAnchor,\n",
    "    FeatureQuery,\n",
    "    HdfsSource,\n",
    "    MaterializationSettings,\n",
    "    ObservationSettings,\n",
    "    RedisSink,\n",
    "    TypedKey,\n",
    "    ValueType,\n",
    "    WindowAggTransformation,\n",
    ")\n",
    "from feathr.datasets.utils import maybe_download\n",
    "from feathr.utils.config import generate_config\n",
    "from feathr.utils.job_utils import get_result_df\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "PATH_TO_APP_DATA = \"hdfs://namenode:9000/data\"\n",
    "\n",
    "print(f\"Feathr version: {feathr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2235d3",
   "metadata": {},
   "source": [
    "#### SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "358c9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"write-synthetic-parquet-to-hdfs\")  # type: ignore[attr-defined]\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c47eb4",
   "metadata": {},
   "source": [
    "#### Feathr client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e43dfb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 13:12:18.992 | INFO     | feathr.utils._env_config_reader:get:60 - Config secrets__azure_key_vault__name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-25 13:12:18.996 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__s3__s3_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-25 13:12:18.997 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__adls__adls_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-25 13:12:18.998 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__wasb__wasb_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-25 13:12:18.999 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__jdbc__jdbc_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-25 13:12:19.000 | INFO     | feathr.utils._env_config_reader:get:60 - Config offline_store__snowflake__snowflake_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "WARNING:feathr.client:No offline storage enabled.\n",
      "2026-01-25 13:12:19.003 | INFO     | feathr.utils._env_config_reader:get:60 - Config spark_config__local__workspace is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-25 13:12:19.003 | INFO     | feathr.utils._env_config_reader:get:60 - Config feature_registry__purview__purview_name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2026-01-25 13:12:19.005 | INFO     | feathr.client:__init__:216 - Feathr client 1.0.0 initialized successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found jar file at ./feathr_2.12-1.0.0.jar\n"
     ]
    }
   ],
   "source": [
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "os.environ['REDIS_PASSWORD'] = \"\"\n",
    "\n",
    "jar_name = glob.glob(\"./*.jar\")[0]\n",
    "print(f\"Found jar file at {jar_name}\")\n",
    "\n",
    "feathr_workspace_folder = Path(\"./feathr_config.yaml\")\n",
    "\n",
    "client = FeathrClient(str(feathr_workspace_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35af48f",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601dcab",
   "metadata": {},
   "source": [
    "### Upload quick start data to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae73fa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== user_observation ======\n",
      "+-------+----------+---------------+--------------+\n",
      "|user_id|product_id|event_timestamp|product_rating|\n",
      "+-------+----------+---------------+--------------+\n",
      "|     u1|        p1|     2023-01-01|             5|\n",
      "|     u1|        p2|     2023-01-02|             4|\n",
      "|     u2|        p1|     2023-01-03|             3|\n",
      "|     u2|        p3|     2023-01-04|             5|\n",
      "|     u3|        p2|     2023-01-05|             2|\n",
      "+-------+----------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quick_start_data_list = !ls feathr_data/\n",
    "\n",
    "for i in quick_start_data_list:\n",
    "    df_name = i.split(\".\")[0]\n",
    "    hdfs_path = f\"{PATH_TO_APP_DATA}/{df_name}\"\n",
    "\n",
    "    df = spark.createDataFrame(pd.read_csv(f\"feathr_data/{i}\"))\n",
    "    df.repartition(1).write.mode(\"overwrite\").parquet(hdfs_path)\n",
    "    \n",
    "    last_path = hdfs_path.split(\"/\")[-1]\n",
    "\n",
    "    if \"observation\" in last_path:\n",
    "        print(f\"====== {last_path} ======\")\n",
    "        spark.read.parquet(f\"{hdfs_path}\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1a218",
   "metadata": {},
   "source": [
    "### Try Feathr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08643b99",
   "metadata": {},
   "source": [
    "#### Define Feathr source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ccee1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_source = HdfsSource(\n",
    "    name=\"user_observation\",\n",
    "    path=f\"{PATH_TO_APP_DATA}/user_observation\",\n",
    "    event_timestamp_column=\"event_timestamp\",\n",
    "    timestamp_format=\"yyyy-MM-dd\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6cfd07",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90caea44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d2bcf24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03aa66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d8933d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab656807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-store-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
